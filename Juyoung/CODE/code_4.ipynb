{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "OliaDaX_lwou"
   },
   "source": [
    "# **ğŸ“„ Document type classification baseline code**\n",
    "> ë¬¸ì„œ íƒ€ì… ë¶„ë¥˜ ëŒ€íšŒì— ì˜¤ì‹  ì—¬ëŸ¬ë¶„ í™˜ì˜í•©ë‹ˆë‹¤! ğŸ‰     \n",
    "> ì•„ë˜ baselineì—ì„œëŠ” ResNet ëª¨ë¸ì„ ë¡œë“œí•˜ì—¬, ëª¨ë¸ì„ í•™ìŠµ ë° ì˜ˆì¸¡ íŒŒì¼ ìƒì„±í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.\n",
    "\n",
    "## Contents\n",
    "- Prepare Environments\n",
    "- Import Library & Define Functions\n",
    "- Hyper-parameters\n",
    "- Load Data\n",
    "- Train Model\n",
    "- Inference & Save File\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zkH9T_86lDSS"
   },
   "source": [
    "## 1. Prepare Environments\n",
    "\n",
    "* ë°ì´í„° ë¡œë“œë¥¼ ìœ„í•œ êµ¬ê¸€ ë“œë¼ì´ë¸Œë¥¼ ë§ˆìš´íŠ¸í•©ë‹ˆë‹¤.\n",
    "* í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## 2. Import Library & Define Functions\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•¨ìˆ˜ì™€ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "from sklearn.model_selection import KFold\n",
    "from torch import nn, optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1700314772722,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ í´ë˜ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, path, transform=None):\n",
    "        self.df = pd.read_csv(csv).values\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)))\n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch í•™ìŠµì„ ìœ„í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        preds = model(image)\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret\n",
    "\n",
    "def validate(loader, model, loss_fn, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, targets in loader:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "            targets_list.extend(targets.detach().cpu().numpy())\n",
    "    \n",
    "    val_loss /= len(loader)\n",
    "    val_acc = accuracy_score(targets_list, preds_list)\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size)\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_loss(loss_fn, pred, labels_a, labels_b, lam):\n",
    "    return lam * loss_fn(pred, labels_a) + (1 - lam) * loss_fn(pred, labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Define Data Augmentation (for specific classes)\n",
    "augment_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "default_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* í•™ìŠµ ë° ì¶”ë¡ ì— í•„ìš”í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë“¤ì„ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "train_path = '../data/train.csv' # train.csv ì´ë¯¸ì§€ í´ë” ë¡œë“œ\n",
    "train_img_path = '../data/train' # train ì´ë¯¸ì§€ í´ë” ë¡œë“œ\n",
    "test_img_path = '../data/test' # test ì´ë¯¸ì§€ í´ë” ë¡œë“œ\n",
    "sub_path = '../data/sample_submission.csv'\n",
    "\n",
    "# model config\n",
    "model_name = 'efficientnet_b3'\n",
    "\n",
    "# training config\n",
    "img_size = 224\n",
    "LR = 1e-3\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "num_workers = 0\n",
    "log_interval = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. (Offline) Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import albumentations as A\n",
    "import matplotlib.pyplot as plt \n",
    "from glob import glob \n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = []\n",
    "targets = []\n",
    "data_path = train_img_path\n",
    "\n",
    "def save_rotated_image(image_path, angle, prefix, ID, target):\n",
    "    rotated_id = f'{prefix}_{ID}'\n",
    "    ids.append(rotated_id)\n",
    "    targets.append(target)\n",
    "    rotated_image = np.array(ImageOps.exif_transpose(Image.open(image_path).rotate(angle, expand=True)))\n",
    "    Image.fromarray(rotated_image).save(os.path.join(data_path, rotated_id))\n",
    "\n",
    "for index, ID, target in tqdm(df.itertuples(), desc='Image rotation', mininterval=0.1):\n",
    "    image_path = os.path.join(data_path, ID)\n",
    "    \n",
    "    # Rotate 90, 180, and 270 degrees\n",
    "    save_rotated_image(image_path, 90, 'r90', ID, target)\n",
    "    save_rotated_image(image_path, 180, 'r180', ID, target)\n",
    "    save_rotated_image(image_path, 270, 'r270', ID, target)\n",
    " \n",
    "rotate_data = {\n",
    "    'ID': ids,\n",
    "    'target': targets\n",
    "}\n",
    "rotate_df = pd.DataFrame(rotate_data)    \n",
    "df = pd.concat([df, rotate_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_flip = A.HorizontalFlip(always_apply=True, p=1)\n",
    "v_flip = A.VerticalFlip(always_apply=True, p=1)\n",
    "\n",
    "ids = []\n",
    "targets = []\n",
    "data_path = train_img_path\n",
    "\n",
    "def save_flipped_image(image, flip_transform, prefix, ID, target):\n",
    "    flipped_id = f'{prefix}_{ID}'\n",
    "    ids.append(flipped_id)\n",
    "    targets.append(target)\n",
    "    flipped_image = flip_transform(image=image)['image']\n",
    "    Image.fromarray(flipped_image).save(os.path.join(data_path, flipped_id))\n",
    "\n",
    "for index, ID, target in tqdm(df.itertuples(), desc='Image flip', mininterval=0.1):\n",
    "    image_path = os.path.join(data_path, ID)\n",
    "    image = np.array(Image.open(image_path))\n",
    "\n",
    "    # Vertical flip for r90 or r270, Horizontal flip otherwise\n",
    "    if ID.startswith('r90') or ID.startswith('r270'):\n",
    "        save_flipped_image(image, v_flip, 'Vflip', ID, target)\n",
    "    else:\n",
    "        save_flipped_image(image, h_flip, 'Hflip', ID, target)\n",
    "\n",
    "# Dataframe for flipped images\n",
    "flip_data = {\n",
    "    'ID': ids,\n",
    "    'target': targets\n",
    "}\n",
    "flip_df = pd.DataFrame(flip_data)    \n",
    "df = pd.concat([df, flip_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = A.Compose([\n",
    "    A.OneOf([\n",
    "        A.GridDistortion(num_steps=5, distort_limit=0.3, interpolation=1, border_mode=4, value=None, mask_value=None, always_apply=True, p=1),\n",
    "        A.ElasticTransform(always_apply=True, p=1, alpha=1.0, sigma=50.0, alpha_affine=50.0, interpolation=0, border_mode=1, value=(0, 0, 0), mask_value=None, approximate=False),\n",
    "        A.OpticalDistortion(always_apply=True, p=1, distort_limit=(-0.3, -0.1)),\n",
    "        A.OpticalDistortion(always_apply=True, p=1, distort_limit=(0.1, 0.3)),\n",
    "    ], p=0.85),\n",
    "    A.SomeOf([\n",
    "        A.RandomBrightnessContrast(brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), p=1),\n",
    "        A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=25, val_shift_limit=20, p=1),\n",
    "        A.MultiplicativeNoise(p=1, multiplier=(1, 1.5), per_channel=True),\n",
    "        A.Equalize(p=1, mode='cv', by_channels=True),\n",
    "    ], n=2, p=0.85),\n",
    "    A.OneOf([\n",
    "        A.Rotate(limit=(10, 30), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "        A.Rotate(limit=(150, 170), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "        A.Rotate(limit=(190, 210), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "        A.Rotate(limit=(330, 350), border_mode=cv2.BORDER_CONSTANT, p=1),\n",
    "    ], p=1),\n",
    "    A.CoarseDropout(p=0.5, max_holes=40, max_height=15, max_width=15, min_holes=8, min_height=8, min_width=8),\n",
    "    A.Equalize(p=0.5, mode='cv', by_channels=True),\n",
    "    A.OneOf([\n",
    "        A.Blur(blur_limit=(3, 4), p=1),\n",
    "        A.MotionBlur(blur_limit=(3, 5), p=1),\n",
    "        A.Downscale(scale_min=0.455, scale_max=0.5, interpolation=2, p=1),\n",
    "    ], p=0.5),\n",
    "    A.GaussNoise(var_limit=(100, 800), per_channel=True, p=0.5),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmentation count based on target\n",
    "def get_augmentation_count(target):\n",
    "    if target == 13:\n",
    "        return 10\n",
    "    elif target == 14:\n",
    "        return 10\n",
    "    elif target == 1:\n",
    "        return 10\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "ids = []\n",
    "targets = []\n",
    "data_path = train_img_path\n",
    "\n",
    "for index, ID, target in tqdm(df.itertuples(), desc='Image augmentation', mininterval=0.1):\n",
    "    image_path = os.path.join(data_path, ID)\n",
    "    image = np.array(Image.open(image_path))\n",
    "    n = get_augmentation_count(target)\n",
    "    \n",
    "    for i in range(n):\n",
    "        transformed_image = transforms(image=image)['image']\n",
    "        image_ID = f'tf{i}_{ID}' \n",
    "        ids.append(image_ID)\n",
    "        targets.append(target)\n",
    "        Image.fromarray(transformed_image).save(os.path.join(data_path, image_ID))\n",
    "    \n",
    "# Create augmented DataFrame and concatenate with original\n",
    "aug_data = {\n",
    "    'ID': ids,\n",
    "    'target': targets\n",
    "}\n",
    "aug_df = pd.DataFrame(aug_data)\n",
    "df = pd.concat([df, aug_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/train2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_path = '../data/train2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/root/CV_PJT/CV_PJT/code/train2.csv')\n",
    "sample_submission_df = pd.read_csv(sub_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* í•™ìŠµ, í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ê³¼ ë¡œë”ë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "llh5C7ZKoq2S"
   },
   "outputs": [],
   "source": [
    "# augmentationì„ ìœ„í•œ transform ì½”ë“œ\n",
    "trn_transform = A.Compose([\n",
    "    # ì´ë¯¸ì§€ í¬ê¸° ì¡°ì •\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    # images normalization\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # numpy ì´ë¯¸ì§€ë‚˜ PIL ì´ë¯¸ì§€ë¥¼ PyTorch í…ì„œë¡œ ë³€í™˜\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "# test image ë³€í™˜ì„ ìœ„í•œ transform ì½”ë“œ\n",
    "tst_transform = A.Compose([\n",
    "    A.Resize(height=img_size, width=img_size),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset definition\n",
    "full_dataset = ImageDataset(\n",
    "    train2_path,\n",
    "    train_img_path,\n",
    "    transform=trn_transform\n",
    ")\n",
    "\n",
    "# Calculate the total number of samples in the dataset\n",
    "dataset_size = len(full_dataset)\n",
    "\n",
    "# Define the ratios for training and validation\n",
    "train_ratio = 0.8  # Use 80% of the data for training\n",
    "val_ratio = 1 - train_ratio  # Remaining 20% for validation\n",
    "\n",
    "# Calculate the number of samples for training and validation\n",
    "train_size = int(train_ratio * dataset_size)\n",
    "val_size = dataset_size - train_size  # Ensure all samples are accounted for\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "trn_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "# Define the test dataset\n",
    "tst_dataset = ImageDataset(\n",
    "    sub_path,\n",
    "    test_img_path,\n",
    "    transform=tst_transform\n",
    ")\n",
    "\n",
    "# Print the sizes of the datasets\n",
    "print(\"Training dataset size:\", len(trn_dataset))\n",
    "print(\"Validation dataset size:\", len(val_dataset))\n",
    "print(\"Test dataset size:\", len(tst_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315112808,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "_sO03fWaQj1h"
   },
   "outputs": [],
   "source": [
    "# DataLoader ì •ì˜\n",
    "trn_loader = DataLoader(\n",
    "    trn_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")\n",
    "tst_loader = DataLoader(\n",
    "    tst_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train Model\n",
    "* ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , í•™ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 870,
     "status": "ok",
     "timestamp": 1700315114067,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "FbBgFPsLT-CO"
   },
   "outputs": [],
   "source": [
    "# load model\n",
    "model = timm.create_model(\n",
    "    model_name,\n",
    "    pretrained=True,\n",
    "    num_classes=17\n",
    ").to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device=device)\n",
    "    ret['epoch'] = epoch\n",
    "\n",
    "    log = \"\"\n",
    "    for k, v in ret.items():\n",
    "      log += f\"{k}: {v:.4f}\\n\"\n",
    "    print(log)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ì— ëŒ€í•œ ì¶”ë¡ ì„ ì§„í–‰í•˜ê³ , ê²°ê³¼ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99/99 [00:14<00:00,  6.91it/s]\n"
     ]
    }
   ],
   "source": [
    "preds_list = []\n",
    "\n",
    "model.eval()\n",
    "for image, _ in tqdm(tst_loader):\n",
    "    image = image.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        preds = model(image)\n",
    "    preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1700315216829,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "aClN7Qi7VZoh"
   },
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame(tst_dataset.df, columns=['ID', 'target'])\n",
    "pred_df['target'] = preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1700315238836,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "VDBXQqAzVvLY"
   },
   "outputs": [],
   "source": [
    "sample_submission_df = pd.read_csv(sub_path)\n",
    "assert (sample_submission_df['ID'] == pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 317,
     "status": "ok",
     "timestamp": 1700315244710,
     "user": {
      "displayName": "Ynot(ì†¡ì›í˜¸)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "ePx2vCELVnuS"
   },
   "outputs": [],
   "source": [
    "pred_df.to_csv(\"pred.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Count the number of predictions per class\n",
    "sns.countplot(x='target', data=pred_df, palette='Set2')\n",
    "plt.title('Distribution of Predicted Classes')\n",
    "plt.xlabel('Class Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
